---
title: "Predicting Total Wins Per Season in Major Leauge Baseball from Game Statistics"
author: Daniel Brooks (daniel.brooks@spsmail.cuny.edu), Daniel Fanelli (daniel.fanelli@spsmail.cuny.edu),
  Christopher Fenton (christopher.fenton@spsmail.cuny.edu), James Hamski (james.hamski@spsmail.cuny.edu),
  Youqing Xiang (youqing.xiang@spsmail.cuny.edu)
date: "6/19/2016"
output:
  pdf_document:
    fig_caption: no
    keep_tex: yes
    number_sections: yes
  html_document:
    fig_caption: no
    force_captions: yes
    highlight: pygments
    number_sections: yes
    theme: cerulean
csl: report_formatting.csl
---
```{r, echo=FALSE, warning=FALSE, message=FALSE}
require(ggplot2)
require(grid)
require(gridExtra)
require(dplyr)
require(knitr)

require(corrplot)
require(caret)
require(nortest)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
train <- read.csv("moneyball-training-data.csv")

train <- select(train, -INDEX)
```
# Introduction
Baseballl is a sport that follows a sequence of pitches, at-bats, and innings where play is contained between discrete pitches. Unlike the more continuous play of soccer or basketball, this makes baseball conducive to gathering extensive data on individual and team performance.   

In this report we attempt to model wins per season for Major League Baseball (MLB) teams (response variable). Our dataset includes 15 potential predictor variables, adjusted to reflect a standardized 162 game season, using MLB records from 1871 to 2006. 

# Data Exploration

## Response Variable
Team Wins (TARGET_WINS) appears to be normally distributed with a slight left skew and a mean of 80.79, which is half of the total 162 game season.   
```{r, echo=FALSE, fig.width=3, fig.height=3, cache=TRUE}
ggplot(train, aes(x=TARGET_WINS)) + geom_density()
```

## Predictor Variables

Most of the predictor variables appear to be approximately normally distributed. Interesesting results include:

Homeruns (TEAM_BATTING_HR) appears to be multinomial. Because the dataset contains game results from 1871 to 2006, it includes time periods which are known to have influenced the occurance of homeruns, including "The Steriod Era" and the introduction of the designated hitter in the American League [Greenberg, N. 2016](https://www.washingtonpost.com/news/fancy-stats/wp/2016/03/07/the-perfect-storm-that-created-baseballs-biggest-home-run-surge-since-the-steroid-era/). Batting Strikeouts (TEAM_BATTING_STRIKEOUTS) also appears multinomial.  

*Histograms indicating the distribution of each variable*    
```{r, echo=FALSE, warning=FALSE, message=FALSE, eval=T, cache=TRUE}
# TARGET_WINS dot plotted with each field:
plt_TEAM_BATTING_H <- ggplot(train, aes(x=TEAM_BATTING_H)) + geom_density()
plt_TEAM_BATTING_2B <- ggplot(train, aes(x=TEAM_BATTING_2B)) + geom_density()
plt_TEAM_BATTING_3B <- ggplot(train, aes(x=TEAM_BATTING_3B)) + geom_density()
plt_TEAM_BATTING_HR <- ggplot(train, aes(x=TEAM_BATTING_HR)) + geom_density()
plt_TEAM_BATTING_BB <- ggplot(train, aes(x=TEAM_BATTING_BB)) + geom_density()
plt_TEAM_BATTING_HBP <- ggplot(train, aes(x=TEAM_BATTING_HBP)) + geom_density()
plt_TEAM_BATTING_SO <- ggplot(train, aes(x=TEAM_BATTING_SO)) + geom_density()
plt_TEAM_BASERUN_SB <- ggplot(train, aes(x=TEAM_BASERUN_SB)) + geom_density()
plt_TEAM_BASERUN_CS <- ggplot(train, aes(x=TEAM_BASERUN_CS)) + geom_density()
plt_TEAM_FIELDING_E <- ggplot(train, aes(x=TEAM_FIELDING_E)) + geom_density()
plt_TEAM_FIELDING_DP <- ggplot(train, aes(x=TEAM_FIELDING_DP)) + geom_density()
plt_TEAM_PITCHING_BB <- ggplot(train, aes(x=TEAM_PITCHING_BB)) + geom_density()
plt_TEAM_PITCHING_H <- ggplot(train, aes(x=TEAM_PITCHING_H)) + geom_density()
plt_TEAM_PITCHING_HR <- ggplot(train, aes(x=TEAM_PITCHING_HR)) + geom_density()
plt_TEAM_PITCHING_SO <- ggplot(train, aes(x=TEAM_PITCHING_SO)) + geom_density()

grid.arrange(plt_TEAM_BATTING_H, plt_TEAM_BATTING_2B, plt_TEAM_BATTING_3B, plt_TEAM_BATTING_HR, plt_TEAM_BATTING_BB, plt_TEAM_BATTING_HBP, plt_TEAM_BATTING_SO, plt_TEAM_BASERUN_SB, plt_TEAM_BASERUN_CS, plt_TEAM_FIELDING_E, plt_TEAM_FIELDING_DP, plt_TEAM_PITCHING_BB, plt_TEAM_PITCHING_H, plt_TEAM_PITCHING_HR, plt_TEAM_PITCHING_SO,   ncol = 3, nrow = 5)
```


The only variables which appear to be positively correlated with Team Wins over their entire range are Hits by Batters (TEAM_BATTING_H) and Doubles by Batters (TEAM_BATTING_2B). Errors (TEAM_FIELDING_E) is negatively correlated with wins at it's larger values. For the rest of the predictor variables, a smoothed conditional mean indicates a trend when plotted against Team Wins only at extreme high or low values where data points are sparse, or no trend at all.  

*Predictor variables plotted versus Team Wins, including a smoothed conditional mean*  
```{r, echo=FALSE, warning=FALSE, message=FALSE, eval=T, cache=TRUE}
# TARGET_WINS dot plotted with each field:
plt_TEAM_BATTING_H <- ggplot(train, aes(x=TEAM_BATTING_H, y=TARGET_WINS)) + geom_point() + geom_smooth() + ylab("WINS")
plt_TEAM_BATTING_2B <- ggplot(train, aes(x=TEAM_BATTING_2B, y=TARGET_WINS)) + geom_point() + geom_smooth() + ylab("WINS")
plt_TEAM_BATTING_3B <- ggplot(train, aes(x=TEAM_BATTING_3B, y=TARGET_WINS)) + geom_point() + geom_smooth() + ylab("WINS")
plt_TEAM_BATTING_HR <- ggplot(train, aes(x=TEAM_BATTING_HR, y=TARGET_WINS)) + geom_point() + geom_smooth() + ylab("WINS")
plt_TEAM_BATTING_BB <- ggplot(train, aes(x=TEAM_BATTING_BB, y=TARGET_WINS)) + geom_point() + geom_smooth() + ylab("WINS")
plt_TEAM_BATTING_HBP <- ggplot(train, aes(x=TEAM_BATTING_HBP, y=TARGET_WINS)) + geom_point() + geom_smooth() + ylab("WINS")
plt_TEAM_BATTING_SO <- ggplot(train, aes(x=TEAM_BATTING_SO, y=TARGET_WINS)) + geom_point() + geom_smooth() + ylab("WINS")
plt_TEAM_BASERUN_SB <- ggplot(train, aes(x=TEAM_BASERUN_SB, y=TARGET_WINS)) + geom_point() + geom_smooth() + ylab("WINS")
plt_TEAM_BASERUN_CS <- ggplot(train, aes(x=TEAM_BASERUN_CS, y=TARGET_WINS)) + geom_point() + geom_smooth() + ylab("WINS")
plt_TEAM_FIELDING_E <- ggplot(train, aes(x=TEAM_FIELDING_E, y=TARGET_WINS)) + geom_point() + geom_smooth() + ylab("WINS")
plt_TEAM_FIELDING_DP <- ggplot(train, aes(x=TEAM_FIELDING_DP, y=TARGET_WINS)) + geom_point() + geom_smooth() + ylab("WINS")
plt_TEAM_PITCHING_BB <- ggplot(train, aes(x=TEAM_PITCHING_BB, y=TARGET_WINS)) + geom_point() + geom_smooth() + ylab("WINS")
plt_TEAM_PITCHING_H <- ggplot(train, aes(x=TEAM_PITCHING_H, y=TARGET_WINS)) + geom_point() + geom_smooth() + ylab("WINS")
plt_TEAM_PITCHING_HR <- ggplot(train, aes(x=TEAM_PITCHING_HR, y=TARGET_WINS)) + geom_point() + geom_smooth() + ylab("WINS")
plt_TEAM_PITCHING_SO <- ggplot(train, aes(x=TEAM_PITCHING_SO, y=TARGET_WINS)) + geom_point() + geom_smooth() + ylab("WINS")


grid.arrange(plt_TEAM_BATTING_H, plt_TEAM_BATTING_2B, plt_TEAM_BATTING_3B, plt_TEAM_BATTING_HR, plt_TEAM_BATTING_BB, plt_TEAM_BATTING_HBP, plt_TEAM_BATTING_SO, plt_TEAM_BASERUN_SB, plt_TEAM_BASERUN_CS, plt_TEAM_FIELDING_E, plt_TEAM_FIELDING_DP, plt_TEAM_PITCHING_BB, plt_TEAM_PITCHING_H, plt_TEAM_PITCHING_HR, plt_TEAM_PITCHING_SO,   ncol = 3, nrow = 5)
```


# Data Preparation

## Outliers and Non-sensical Values  

The histograms and scatterplots above indicate a few variables which have outliers or non-sensical values.   

#### Base Hits by Batters, Doubles, Triples, Homeruns  

No changes made.  

#### Strikeouts by Batters   
Several 0 values were replaced with NAs, as it is virtually impossible to make it through an entire season with zero strike outs. The next low value (66) indicates 0 is missing data in this variable.  

```{r, echo=FALSE}
train$TEAM_BATTING_SO[train$TEAM_BATTING_SO == 0] <- NA
```

#### Hit by Pitch   
See **Missing Data** section below.    

#### Strikeout by Batters, Stolen bases, Caught Stealing, Errors, Double Plays 

No changes made.  

#### Errors, Walks Allowed, Strikeouts by Pitchers    
Two outlier values for TEAM_PITCHING_SO are higher than the total number of out during a season excluding extra innings (4,374). In addition, these series tended to have unreasonably long right skewed tails. Therefore, high outliers, as defnited by three standard deviations from the mean, were replaced with NAs.  

```{r, echo=FALSE}
train$TEAM_FIELDING_E[train$TEAM_FIELDING_E > (sd(train$TEAM_FIELDING_E, na.rm=T)*3 + mean(train$TEAM_FIELDING_E))] <- NA
train$TEAM_PITCHING_BB[train$TEAM_PITCHING_BB > (sd(train$TEAM_PITCHING_BB, na.rm=T)*3 + mean(train$TEAM_PITCHING_BB))] <- NA
train$TEAM_PITCHING_SO[train$TEAM_PITCHING_SO > (sd(train$TEAM_PITCHING_SO, na.rm=T)*3 + mean(train$TEAM_PITCHING_SO, na.rm=T))] <- NA
```

#### Hits Allowed
We would expect the maximum Hits Allowed (TEAM_PITCHING_H) to be on par with the maximum Hits by Batters (TEAM_BATTING_H). However, Hits allowed has many values that are thousands higher than Hits by Batters. Therefore, Hits Allowed greater than the maximum Hits by Batters were replaced with NAs. 
```{r, echo=FALSE}
train$TEAM_PITCHING_H[train$TEAM_PITCHING_H > max(train$TEAM_BATTING_H)] <- NA
```

```{r, echo=FALSE}
train <- select(train, -TEAM_BATTING_HBP)
```

## Multicollinearity  
One of the challenges of this dataset is the existence of variables that are by-definition correlated. For a complete dataset for the variables here, for all teams in MLB, several variables will have common sums. Dy definition: for every one Hit by Batters there will be one Hit Allowed. This is the case for: Hits (singles through homeruns), strikeouts and walks. Teams tend to play within their leauge (American Leauge / National Leauge) and within their division frequently. This is perhaps an explanation for the existence of collinearity in the dataset.    
  
In addition, some variables are indicators of frequency attempted. Caught stealing and stolen bases are highly correlated by team. They're also correlated by individual player - Ricky Henderson holds the MLB record for stolen bases at 1,406 - but he also holds the record for most times caught stealing, at 335.
```{r, echo=FALSE, warning=FALSE, fig.height=3, fig.width=3, cache=TRUE}
ggplot(train, aes(x = TEAM_BASERUN_SB, y = TEAM_BASERUN_CS)) + geom_point(aes(alpha=0.25)) + geom_smooth()
```


*Correlation plot for the indicator and all predictor variables*  
```{r, echo=FALSE}
#correlation table
#cor.table <- cor(train, use = "pairwise.complete.obs")
#corrplot(cor.table, diag = FALSE, number.font = 9, type="lower")
```

## Missing Data  

In statistical analysis, it is important remain mindful of context and not ignore the mechanics of the system being studied. Hit-by-Pitch is missing 2085 records - 90% of the dataset. This variable was dropped completely from the dataset and ignored by future analysis for two reasons (1) the vast majority of records were missing and (2) hit-by-pitch is a random event that happens to a team (a team cannot be 'good at being hit by pitches'), therefore it is not expected to be an indicator of total wins.  

The next highest missing value is Caught Stealing, with 33% of the records missing. We decided to test out two different approaches for handling missing data. One was to impute missing values based on the existing values, while the other approach ignored all incomplete records by keeping NAs intact.  

### Dealing with NAs - Imputing from Probability Distributions  

Several variables have missing values (NAs), either from the original dataset or from the elimination of outliers.  
```{r, echo=FALSE}
na.count <- NULL
for(i in 1:ncol(train)){
  na.count <- c(na.count, sum(is.na(train[,i])))
}
na.table <- cbind(colnames(train), na.count)
kable(na.table )
```


For the following predictor variables we imputed missing values by sampling from a normal distribution parameterized by the present data for that variable:  
* TEAM_BASERUN_CS  
* TEAM_BATTING_SO  
* TEAM_BASERUN_SB   
* TEAM_PITCHING_SO  
* TEAM_FIELDING_DP  
* TEAM_PITCHING_H  
* TEAM_PITCHING_BB  
* TEAM_FIELDING_E  


```{r, echo=FALSE, warning=FALSE, message=FALSE}

train.fill.nas <- train

### 2.2 TEAM_BASERUN_CS
train.fill.nas$TEAM_BASERUN_CS[is.na(train.fill.nas$TEAM_BASERUN_CS)] <- sample(train.fill.nas$TEAM_BASERUN_CS[!is.na(train.fill.nas$TEAM_BASERUN_CS)],sum(is.na(train.fill.nas$TEAM_BASERUN_CS)),replace=F)

### 2.3 TEAM_BATTING_SO


# Check normal distribution
TEAM_BATTING_SO <- train.fill.nas$TEAM_BATTING_SO
TEAM_BATTING_SO <- TEAM_BATTING_SO[!is.na(TEAM_BATTING_SO)]
#hist(TEAM_BATTING_SO)
#qqnorm(TEAM_BATTING_SO)
#length(TEAM_BATTING_SO)
#shapiro.test(TEAM_BATTING_SO)

#ad.test(TEAM_BATTING_SO)

# Sampling method to replace NA's
train.fill.nas$TEAM_BATTING_SO[is.na(train.fill.nas$TEAM_BATTING_SO)] <- sample(train.fill.nas$TEAM_BATTING_SO[!is.na(train.fill.nas$TEAM_BATTING_SO)],sum(is.na(train.fill.nas$TEAM_BATTING_SO)),replace=F)

### 2.4 TEAM_BASERUN_SB
# Sampling the value from column
train.fill.nas$TEAM_BASERUN_SB[is.na(train.fill.nas$TEAM_BASERUN_SB)] <- sample(train.fill.nas$TEAM_BASERUN_SB[!is.na(train.fill.nas$TEAM_BASERUN_SB)],sum(is.na(train.fill.nas$TEAM_BASERUN_SB)),replace=F)

### 2.5 TEAM_PITCHING_SO
# Sampling the value from column
train.fill.nas$TEAM_PITCHING_SO[is.na(train.fill.nas$TEAM_PITCHING_SO)] <- sample(train.fill.nas$TEAM_PITCHING_SO[!is.na(train.fill.nas$TEAM_PITCHING_SO)],sum(is.na(train.fill.nas$TEAM_PITCHING_SO)),replace=F)

### 2.6 TEAM_FIELDING_DP
# Check normal distribution
TEAM_FIELDING_DP <- train.fill.nas$TEAM_FIELDING_DP
TEAM_FIELDING_DP <- TEAM_FIELDING_DP[!is.na(TEAM_FIELDING_DP)]
#hist(TEAM_FIELDING_DP)
#qqnorm(TEAM_FIELDING_DP)
#length(TEAM_FIELDING_DP)
#shapiro.test(TEAM_FIELDING_DP)

#ad.test(TEAM_FIELDING_DP)

# Sampling the value from column
train.fill.nas$TEAM_FIELDING_DP[is.na(train.fill.nas$TEAM_FIELDING_DP)] <- sample(train.fill.nas$TEAM_FIELDING_DP[!is.na(train.fill.nas$TEAM_FIELDING_DP)],sum(is.na(train.fill.nas$TEAM_FIELDING_DP)),replace=F)

#Pitching - Hits

train.fill.nas$TEAM_PITCHING_H[is.na(train.fill.nas$TEAM_PITCHING_H)] <- sample(train.fill.nas$TEAM_PITCHING_H[!is.na(train.fill.nas$TEAM_PITCHING_H)],sum(is.na(train.fill.nas$TEAM_PITCHING_H)),replace=F)

# Pitching - BB
train.fill.nas$TEAM_PITCHING_BB[is.na(train.fill.nas$TEAM_PITCHING_BB)] <- sample(train.fill.nas$TEAM_PITCHING_BB[!is.na(train.fill.nas$TEAM_PITCHING_BB)],sum(is.na(train.fill.nas$TEAM_PITCHING_BB)),replace=F)

# Fielding Errors
train.fill.nas$TEAM_FIELDING_E[is.na(train.fill.nas$TEAM_FIELDING_E)] <- sample(train.fill.nas$TEAM_FIELDING_E[!is.na(train.fill.nas$TEAM_FIELDING_E)],sum(is.na(train.fill.nas$TEAM_FIELDING_E)),replace=F)

```


#### Dealing with NAs - Eliminating non-complete cases (ignoring them)  

The rationale for ignoring all but complete records was twofold: first, we wanted to create derived values and thought it better to avoid the complexities introduced by imputation (any incorrect imputation assumptions would be compounded by their use in a derived value). Second, we did know the nature of the missing values, furthering the difficulties of any assumption about their nature.

```{r, echo=FALSE}
train.with.caught.stealing <- train
train <- select(train, -TEAM_BASERUN_CS)
```


## Calculating Base Hits

The column recording Hits by Batters (TEAM_BATTING_H) was flagged as being a potential source of unidentifiablility, because it is composed of the sum of three additional columns: Doubles by Batters (TEAM_BATTING_2B), Triples by Batters (TEAM_BATTING_3B), and Homeruns by Batters(TEAM_BATTING_HR). While Hits by Batters may be have utility in modeling wins on its own, we determined it should not be combined in a model with doubles, triples, and homeruns. Therefore, we subtracted doubles, triples, and home runs from Hits by Batters to create Singles by Batters (TEAM_BATTING_1B).   

Likewise, Hits Allowed was broken into Singles, Doubles and Triples as one variable "TEAM_PITCHING_NON_HR" by subtracting Homeruns Allowed. 

```{r, echo=FALSE}
# go with 1b instead of "hits"
train$TEAM_BATTING_1B <- (train$TEAM_BATTING_H -(train$TEAM_BATTING_2B + train$TEAM_BATTING_3B + train$TEAM_BATTING_HR))
# re-order it:
train <- train[c(ncol(train),2:ncol(train)-1)]

# go with 1_2_3b instead of "hits"
train$TEAM_PITCHING_NON_HR <- (train$TEAM_PITCHING_H - train$TEAM_PITCHING_HR)
train$TEAM_PITCHING_H <- NULL

# re-order it:
train <- train[c(ncol(train),2:ncol(train)-1)]
#colnames(train)
```

```{r, echo=FALSE}
# Again for NA fill

train.fill.nas$TEAM_BATTING_1B <- (train.fill.nas$TEAM_BATTING_H -(train.fill.nas$TEAM_BATTING_2B + train.fill.nas$TEAM_BATTING_3B + train.fill.nas$TEAM_BATTING_HR))
train.fill.nas <- train.fill.nas[c(ncol(train.fill.nas),2:ncol(train.fill.nas)-1)]

train.fill.nas$TEAM_PITCHING_NON_HR <- (train.fill.nas$TEAM_PITCHING_H - train.fill.nas$TEAM_PITCHING_HR)
train.fill.nas$TEAM_PITCHING_H <- NULL

train.fill.nas <- train.fill.nas[c(ncol(train.fill.nas),2:ncol(train.fill.nas)-1)]
```

*Histograms indicating the distribution of each variable after data cleaning*  
```{r, echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
# TARGET_WINS dot plotted with each field:
plt_TEAM_BATTING_1B <- ggplot(train, aes(x=TEAM_BATTING_1B)) + geom_density()
plt_TEAM_BATTING_2B <- ggplot(train, aes(x=TEAM_BATTING_2B)) + geom_density()
plt_TEAM_BATTING_3B <- ggplot(train, aes(x=TEAM_BATTING_3B)) + geom_density()
plt_TEAM_BATTING_HR <- ggplot(train, aes(x=TEAM_BATTING_HR)) + geom_density()
plt_TEAM_BATTING_BB <- ggplot(train, aes(x=TEAM_BATTING_BB)) + geom_density()
plt_TEAM_BATTING_SO <- ggplot(train, aes(x=TEAM_BATTING_SO)) + geom_density()
plt_TEAM_BASERUN_SB <- ggplot(train, aes(x=TEAM_BASERUN_SB)) + geom_density()
plt_TEAM_BASERUN_CS <- ggplot(train, aes(x=TEAM_BASERUN_CS)) + geom_density()
plt_TEAM_FIELDING_E <- ggplot(train, aes(x=TEAM_FIELDING_E)) + geom_density()
plt_TEAM_FIELDING_DP <- ggplot(train, aes(x=TEAM_FIELDING_DP)) + geom_density()
plt_TEAM_PITCHING_BB <- ggplot(train, aes(x=TEAM_PITCHING_BB)) + geom_density()
plt_TEAM_PITCHING_NON_HR <- ggplot(train, aes(x=TEAM_PITCHING_NON_HR)) + geom_density()
plt_TEAM_PITCHING_HR <- ggplot(train, aes(x=TEAM_PITCHING_HR)) + geom_density()
plt_TEAM_PITCHING_SO <- ggplot(train, aes(x=TEAM_PITCHING_SO)) + geom_density()

grid.arrange(plt_TEAM_BATTING_1B, plt_TEAM_BATTING_2B, plt_TEAM_BATTING_3B, plt_TEAM_BATTING_HR, plt_TEAM_BATTING_BB, plt_TEAM_BATTING_SO, plt_TEAM_BASERUN_SB, plt_TEAM_FIELDING_E, plt_TEAM_FIELDING_DP, plt_TEAM_PITCHING_BB, plt_TEAM_PITCHING_NON_HR, plt_TEAM_PITCHING_HR, plt_TEAM_PITCHING_SO,   ncol = 3, nrow = 5)
```


```{r, echo=FALSE}
#reorder dataframe


train <- train[c("TARGET_WINS", "TEAM_BATTING_1B", "TEAM_BATTING_2B", "TEAM_BATTING_3B",  "TEAM_BATTING_HR", "TEAM_BATTING_BB", "TEAM_BATTING_SO", "TEAM_BASERUN_SB", "TEAM_PITCHING_NON_HR", "TEAM_PITCHING_HR", "TEAM_PITCHING_BB", "TEAM_PITCHING_SO", "TEAM_FIELDING_E", "TEAM_FIELDING_DP")]

train.fill.nas <- train.fill.nas[c("TARGET_WINS", "TEAM_BATTING_1B", "TEAM_BATTING_2B", "TEAM_BATTING_3B",  "TEAM_BATTING_HR", "TEAM_BATTING_BB", "TEAM_BATTING_SO", "TEAM_BASERUN_SB", "TEAM_BASERUN_CS", "TEAM_PITCHING_NON_HR", "TEAM_PITCHING_HR", "TEAM_PITCHING_BB", "TEAM_PITCHING_SO", "TEAM_FIELDING_E", "TEAM_FIELDING_DP")]

```


# Build Models  

## Model 1: Backwards Selection - NAs in dataset 

For our first model, we used backward selection. In this method we calculate the linear model starting with all predictor variables, then remove the value with the highest P value until we have no signficance values greater than 0.05. 

The model that results from backward selection is: 

$$Team Wins = 60.5 - 0.031singles - 0.080doubles + 0.152triples + 0.129homeruns + 0.151strikeouts.batting + $$  
$$ 0.070stolen.bases + 0.057pitching-non-homeruns - 0.111pitching.walks - $$  
$$ 0.022pitching.strikeouts - 0.119errors - 0.113double.plays$$


```{r, echo=FALSE}
#Separate data into two parts, one for training models and the other for testing models
set.seed(45)
inTrain <- createDataPartition(y=train$TARGET_WINS, p=0.7,list=FALSE)
train1a <- train[inTrain,]
test1a <- train[-inTrain,]
#dim(train1a)
#Backwards Selection - removing the least-significant variable each time
lm1_a <- lm(TARGET_WINS ~ ., data=train1a)
lm1_a <- update(lm1_a,  .~. - TEAM_PITCHING_HR)
lm1_a <- update(lm1_a,  .~. - TEAM_BATTING_SO)
```

```{r, echo=FALSE}
lm.backwards.remove.one <- lm(TARGET_WINS ~ TARGET_WINS + TEAM_BATTING_1B + TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR + TEAM_BATTING_BB + TEAM_BASERUN_SB + TEAM_PITCHING_NON_HR + TEAM_PITCHING_BB + TEAM_PITCHING_SO + TEAM_FIELDING_E + TEAM_FIELDING_DP, data = train)
summary(lm.backwards.remove.one)
```

## Model 1: Backwards Selection - Using imputed values dataset (no NAs)
```{r, echo=FALSE}
#Separate data into two parts, one for training models and the other for testing models
set.seed(45)
inTrain <- createDataPartition(y=train.fill.nas$TARGET_WINS, p=0.7,list=FALSE)
train1b <- train[inTrain,]
test1b <- train[-inTrain,]
#dim(train1b)
#Backwards Selection - removing the least-significant variable each time
lm1_b <- lm(TARGET_WINS ~ ., data=train1b)
lm1_b <- update(lm1_a,  .~. - TEAM_BATTING_SO)
lm1_b <- update(lm1_a,  .~. - TEAM_BASERUN_CS)
lm1_b <- update(lm1_a,  .~. - TEAM_PITCHING_SO)
lm1_b <- update(lm1_a,  .~. - TEAM_PITCHING_NON_HR)
lm1_b <- update(lm1_a,  .~. - TEAM_PITCHING_HR)
lm1_b <- update(lm1_a,  .~. - TEAM_FIELDING_E)
summary(lm1_b)
```




## Model 2:


```{r}
# PCA analysis
Predictor <- train.fill.nas$TARGET_WINS
A <- as.matrix(select(train.fill.nas,-TARGET_WINS))
pca <- princomp(A,center=T,scale.=T)
plot(pca)
summary(pca)
pca <- as.data.frame(pca$scores[,1:5])
train_pca <- cbind(TARGET_WINS=Predictor,pca)
#head(train_pca)
#dim(train_pca)
# Separate data into two parts, one for training models and the other for testing models
set.seed(45)
inTrain_pca <- createDataPartition(y=train_pca$TARGET_WINS, p=0.7,list=FALSE)
training_pca <- train_pca[inTrain_pca,]
testing_pca <- train_pca[-inTrain_pca,]

# Build a model
lm2_a <- lm(TARGET_WINS ~ ., data=training_pca)
summary(lm2_a)
```



## Model 3: Using Variable Ratios 

As discussed above, several variables display collinearity. Some variables by definition have the same sum (for every one batting walk, another team has a pitching walk) and some variables indicate a hidden variable (stolen bases and caught stealing appear to indicate stolen base attempts). Therefore, we used several ratios among predictor variables. 

## Deriving Ratio Variables

One area of interest was whether or not variables relative to another relevant variable would prove to be a better predictor than the raw data on its own. To test this out, we created 3 ratio variables: Stolen base percentage, HR to Strikout (batting) percentage, and Strikeout to Walk (pitching, otherwise known as K/BB) percentage. These ratios were calculated from the dataset that ignored records with missing values (discussed later).

Stolen base percentage measured stolen bases over stolen bases plus caught stealing, which would constitue total stolen base attempts.

HR to Strikout percentage was derived because batting HRs and Strikeouts had a correlation of .6402. Thus perhaps more valuable than knowing the gross amounts of either variable would be the ratio of one to the other.

For pitching, Strikeout to Walk (K/BB) Ratio was calculated. [This is a traditional baseball statistic that has currently come under scrutiny with the modernization of baseball analysis.](http://www.beyondtheboxscore.com/2012/11/25/3686732/stop-using-k-bb) With that in mind we thought it would be of interest to see what kind of impact including this variable would have on a model.   


```{r, eval=TRUE}
cmlb <- train.with.caught.stealing

#add stolen base pct
cmlb$TEAM_BASERUN_SB_PCT <- cmlb$TEAM_BASERUN_SB / (cmlb$TEAM_BASERUN_SB + 
                                                      cmlb$TEAM_BASERUN_CS)

#compare Stolen Base pct to raw stolen bases
summary(lm(TARGET_WINS ~ TEAM_BASERUN_SB_PCT, data=cmlb))
summary(lm(TARGET_WINS ~ TEAM_BASERUN_SB, data=cmlb))

#add HR to Strikeout pct

cmlb$TEAM_BATTING_HR_TO_SO <- cmlb$TEAM_BATTING_HR / cmlb$TEAM_BATTING_SO


#compare HR/SO pct to raw HRs

summary(lm(TARGET_WINS ~ TEAM_BATTING_HR_TO_SO, data=cmlb))
summary(lm(TARGET_WINS ~ TEAM_BATTING_HR, data=cmlb))

plot(cmlb$TEAM_BATTING_HR_TO_SO, cmlb$TARGET_WINS)
plot(cmlb$TEAM_BATTING_HR, cmlb$TARGET_WINS)

#add StrikeOut to walk (K/BB) pct

cmlb$TEAM_PITCHING_SO_TO_BB <- cmlb$TEAM_PITCHING_SO / cmlb$TEAM_PITCHING_BB


#look at K/BB model

summary(lm(TARGET_WINS ~ TEAM_PITCHING_SO_TO_BB, data = cmlb))
plot(cmlb$TEAM_PITCHING_SO_TO_BB, cmlb$TARGET_WINS)
```

  

# Select Model  

## Evaluating Model 1  

```{r, echo=FALSE}
plot(fitted(lm1_a), residuals(lm1_a), xlab = "Fitted", ylab = "Residuals")  
abline(h = 0)
qqnorm(residuals(lm1_a))  
qqline(residuals(lm1_a))
```




```{r, echo=FALSE}
plot(fitted(lm1_b), residuals(lm1_b), xlab = "Fitted", ylab = "Residuals")
abline(h = 0)
qqnorm(residuals(lm1_b))
qqline(residuals(lm1_b))


```
## Evaluating Model 2  

## Evaluating Model 3  

## Evaluating Models (1a,1b,2a)
```{r,echo=FALSE}
fit1a <- aov(lm1_a,data=test1a)
fit1b <- aov(lm1_b,data=test1b)
fit2a <- aov(lm2_a,data=testing_pca)
fit1a;fit1b;fit2a
```
## References  


